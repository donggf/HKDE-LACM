# HKDE-LACM
1.	Introduction
This package implements the "HKDE-LACM: Lactic Acid Bacteria Classification Model based on High-Dimensional k-mers Frequency and DNABERT Embedding". The model analyzes the genomic sequences of lactic acid bacteria, leveraging high-dimensional k-mers frequency features and DNABERT embedding technology to achieve classification of lactic acid bacteria.
2.	Data Sample
The data can be downloaded from the following website:：http://bioinfor.imu.edu.cn/iprobiotics/public/download.html
Please save the samples into the directory data\sample_data\data following the specified format:
•	Positive samples should be named starting with "A" followed by a numeric identifier (e.g., "A1", "A2").
•	Negative samples should be named starting with "B" followed by a numeric identifier (e.g., "B1", "B2").
Refer to the example samples already provided in the directory for guidance.

3.	Environment settings
GPU：NVIDIA RTX 4090D（24GB）× 1;
CPU：AMD EPYC 9754（128 core），18 vCPU;
Memory：60GB;
Operating system：Ubuntu 20.04；
PyTorch：2.0.0;
Python：3.8;
Cuda：11.8

Install Jellyfish on Linux systems:：
Refer to the guide:：Jellyfish安装
Create and activate a virtual environment:：
python3 -m venv pytorch_env
source pytorch_env/bin/activate
Install required packages：
python3 -m pip install -r requirements.txt
4.	Running the Code
Given that fine-tuning the model and extracting embedding sequences may be time-intensive for large-scale datasets, you can choose one of the following two methods to obtain predictions based on your data size and hardware performance：
a)	Run the following Python code files sequentially in the specified order:：
i.	Data Splitting & Preprocessing
Split the dataset into training, testing, and validation sets. Each sample is divided into 512-length segments, and 6-mers features are extracted. Processed results are saved to the “data\sample_data \processed” directory：
Split_dataset.py 
ii.	Model Fine-tuning
Fine-tune the model using the preprocessed data to obtain a fine-tuned model, which will be saved in the”data\sample_data\finetuned_DNABERT6” directory. If a suitable fine-tuned model already exists, you can skip this step:：
Finetuned_model.py
iii.	mbedding Sequence Generation
Use the fine-tuned model to predict genomic sequence segments, generating predicted labels and embedding sequences. Results are saved in the “data\sample_data\embedding_vector”directory：
gain_prediction.py
iv.	Embedding Vector Classification
Categorize embedding sequences into:
•	Positive embedding vectors (predicted labels match true labels)
•	Negative embedding vectors (predicted labels disagree with true labels)
Results are saved in the
“data\sample_data\result” directory.：
Split_forward_and_reverse.py
v.	Aggregate Full Gene Sequence Embedding Vectors
Compile the embedding vectors of gene sequence fragments into complete positive and negative embedding vectors corresponding to the full gene sequences. Results are saved in the “data\sample_data\result” directory with filenames：merged_correct_sequence_embeddings_1024.tsv和merged_incorrect_sequence_embeddings_1024.tsv：
Obtain_complete_embeddings.py
Obtain_incorrect_complete_embeddings.py
vi.	Feature Fusion & Prediction
Integrate the embedding vectors with the k-mers frequency matrix generated by Jellyfish, and perform prediction via grid search optimization. The k-value is provided as input when executing this file. Final predictions are saved in“data\samples\result\prediction.tsv“，where:
•	The first column contains sample names.
•	The second column lists predicted labels.
Predict.py
b)	Execute the script directly with the following command：
bash running.sh
Note: After starting the run, enter the k-value when prompted.
Runtime with 57 Positive and 57 Negative Samples
We provide runtime estimates based on a dataset of 57 positive and 57 negative samples.
1.	k-mers Calculation Time
o	As the k-value increases, the computation time for generating k-mers per sample grows progressively.
o	We currently allocate an 11-second waiting period to fully capture 11-mer sequences from the genomic sequences.
o	Users can adjust the waiting time in the predict.py and predict_sh.py files according to their chosen k-value.
2.	Model Fine-tuning
o	Approximately 6.5 hours are required for model fine-tuning.
3.	Embedding Sequence Extraction
o	Generating embedding sequences for all 114 samples takes around 16.5 hours.
4.	Prediction with Combined Features
Runtime varies with k-values:
o	8-mer: ~8.5 minutes
o	9-mer: ~27 minutes
o	10-mer: ~1 hour 45 minutes
o	11-mer: ~8 hour 20 minutes

